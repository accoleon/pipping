@article{Conery:2005:RWM:1089844.1089845,
 author = {Conery, S. and Catchen, M. and Lynch, Michael},
 title = {Rule-based Workflow Management for Bioinformatics},
 journal = {The VLDB Journal},
 issue_date = {September 2005},
 volume = {14},
 number = {3},
 month = sep,
 year = {2005},
 issn = {1066-8888},
 pages = {318--329},
 numpages = {12},
 url = {http://dx.doi.org/10.1007/s00778-005-0153-9},
 doi = {10.1007/s00778-005-0153-9},
 acmid = {1089845},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Bioinformatics, Rule-based system, Workflow},
}
@Article{21352538,
AUTHOR = {Cieslik, Marcin and Mura, Cameron},
TITLE = {A lightweight, flow-based toolkit for parallel and distributed bioinformatics pipelines},
JOURNAL = {BMC Bioinformatics},
VOLUME = {12},
YEAR = {2011},
NUMBER = {1},
PAGES = {61},
URL = {http://www.biomedcentral.com/1471-2105/12/61},
DOI = {10.1186/1471-2105-12-61},
PubMedID = {21352538},
ISSN = {1471-2105},
ABSTRACT = {BACKGROUND:Bioinformatic analyses typically proceed as chains of data-processing tasks. A pipeline, or 'workflow', is a well-defined protocol, with a specific structure defined by the topology of data-flow interdependencies, and a particular functionality arising from the data transformations applied at each step. In computer science, the dataflow programming (DFP) paradigm defines software systems constructed in this manner, as networks of message-passing components. Thus, bioinformatic workflows can be naturally mapped onto DFP concepts.RESULTS:To enable the flexible creation and execution of bioinformatics dataflows, we have written a modular framework for parallel pipelines in Python ('PaPy'). A PaPy workflow is created from re-usable components connected by data-pipes into a directed acyclic graph, which together define nested higher-order map functions. The successive functional transformations of input data are evaluated on flexibly pooled compute resources, either local or remote. Input items are processed in batches of adjustable size, all flowing one to tune the trade-off between parallelism and lazy-evaluation (memory consumption). An add-on module ('NuBio') facilitates the creation of bioinformatics workflows by providing domain specific data-containers (e.g., for biomolecular sequences, alignments, structures) and functionality (e.g., to parse/write standard file formats).CONCLUSIONS:PaPy offers a modular framework for the creation and deployment of parallel and distributed data-processing workflows. Pipelines derive their functionality from user-written, data-coupled components, so PaPy also can be viewed as a lightweight toolkit for extensible, flow-based bioinformatics data-processing. The simplicity and flexibility of distributed PaPy pipelines may help users bridge the gap between traditional desktop/workstation and grid computing. PaPy is freely distributed as open-source Python code at http://muralab.org/PaPy webcite, and includes extensive documentation and annotated usage examples.},
}
@Article{23046922,
AUTHOR = {Su, Xiaoquan and Xu, Jian and Ning, Kang},
TITLE = {Parallel-META: efficient metagenomic data analysis based on high-performance computation},
JOURNAL = {BMC Systems Biology},
VOLUME = {6},
YEAR = {2012},
NUMBER = {Suppl 1},
PAGES = {S16},
URL = {http://www.biomedcentral.com/1752-0509/6/S1/S16},
DOI = {10.1186/1752-0509-6-S1-S16},
PubMedID = {23046922},
ISSN = {1752-0509},
ABSTRACT = {BACKGROUND:Metagenomics method directly sequences and analyses genome information from microbial communities. There are usually more than hundreds of genomes from different microbial species in the same community, and the main computational tasks for metagenomic data analyses include taxonomical and functional component examination of all genomes in the microbial community. Metagenomic data analysis is both data- and computation- intensive, which requires extensive computational power. Most of the current metagenomic data analysis softwares were designed to be used on a single computer or single computer clusters, which could not match with the fast increasing number of large metagenomic projects' computational requirements. Therefore, advanced computational methods and pipelines have to be developed to cope with such need for efficient analyses.RESULT:In this paper, we proposed Parallel-META, a GPU- and multi-core-CPU-based open-source pipeline for metagenomic data analysis, which enabled the efficient and parallel analysis of multiple metagenomic datasets and the visualization of the results for multiple samples. In Parallel-META, the similarity-based database search was parallelized based on GPU computing and multi-core CPU computing optimization. Experiments have shown that Parallel-META has at least 15 times speed-up compared to traditional metagenomic data analysis method, with the same accuracy of the results http://www.computationalbioenergy.org/parallel-meta.html webcite.CONCLUSION:The parallel processing of current metagenomic data would be very promising: with current speed up of 15 times and above, binning would not be a very time-consuming process any more. Therefore, some deeper analysis of the metagenomic data, such as the comparison of different samples, would be feasible in the pipeline, and some of these functionalities have been included into the Parallel-META pipeline.},
}
@misc{sqlitewhen,
title={Appropriate Uses For SQLite},
howpublished={\url{http://www.sqlite.org/whentouse.html}},
note={Accessed: 2014-06-13}
}
@misc{uparse,
title={UPARSE pipeline},
author={Edgar, Robert},
howpublished={\url{http://www.drive5.com/usearch/manual/uparse_pipeline.html}},
note={Accessed: 2014-06-13}
}
@misc{qiime,
title={454 Overview Tutorial: de novo OTU picking and diversity analyses using 454 data},
howpublished={\url{http://qiime.org/tutorials/tutorial.html}},
note={Accessed: 2014-06-13}
}
@misc{paired-end,
title={Paired-End Sequencing Assay},
howpublished={\url{http://www.illumina.com/technology/next-generation-sequencing/paired-end-sequencing_assay.ilmn}},
note={Accessed: 2014-06-13}
}