
\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\doublespacing
\usepackage[np]{numprint}
\npstyleenglish

\begin{document}
	\title{16S RNA Sequencing Data Management using SQLite}
	\author{Xu Junjie, Kevin}
	\date{June 2014}
	\maketitle
	\begin{abstract}
		The 16S Ribosomal RNA Sequencing is used extensively in analzying bacterial
		phylogeny and taxonomy. This project attempts to streamline the 16S sequencing
		pipeline using local file databases to replace multiple flat sequence files used
		in the pipeline, to ease logistical burdens on the researcher and enable 
		greater metadata analysis and accountability of experiments.
	\end{abstract}
	\tableofcontents
	\section{Introduction} % (fold)
	\label{sec:introduction}
	The project utilized computational methods such as 16S Ribosomal Profiling as a proxy for species identity. The 16S small subunit of bacterial ribosomes are highly conserved, which means that the differences within the 16S RNA profiles can be used as an analogue for species identity. Since the 16S gene contains both highly conserved and highly variable regions all interspersed together,
	The highly conserved regions can be amplified using PCR (Polymerase chain reaction), while the variable regions are used to classify the organism. HTS (High Throughput Sequencing) using Illumina sequencers are then used to sequence the PCR products from the prior step.

	The output from HTS forms the start of the computational pipeline. In the preprocessing stage, poor quality reads are filtered and trimmed. Chimeras and non-bacterial 16S reads are then removed. The pipeline then attempts to produce phylogenetic classification through the use of RDP classifiers and sequence similarity (i.e. OTU Analysis). By comparing the filtered output from HTS against the already existing taxonomies of over 2 million species, the genus, family, and order of the sample can be determined.

	The RDP classifier can fail for various reasons: the majority of bacterial species have not been identified or sequenced, the 16S reads are usually too short for accurate classification. In those cases, groups of highly similar sequences are grouped into OTUs (Operational Taxonomic Unit) that can be analyzed for
	quantitative differences in communities between the sequenced samples.

	The project utilizes a computational approach due to the large amount of raw sequencing data that is created from the PCR amplification process. The variability of results is further affected by the OTU Analysis stage, where prior parameters can affect the analysis outcome. Using a computational approach would allow the change of various parameters, in order to quantitate the effects of those parameter changes.

	The majority of the pipeline is executed on the University of Oregonâ€™s ACISS High-Performance Supercomputer Cluster, and utilizes PBS scripts (normal shell scripts with extra variables defined to manage job resources) to execute the different stages of the pipeline.

	The FASTQ output produced by the Illumina sequencer is run through the preprocessing stage of filtering, trimming, and demultiplexing. The PBS script for the preprocessing stage calls on the Demultiplexer, a python script written by Rodger Voelker, which removes the primer attached to the sequences during the amplification process, and attaches barcodes (signifying sample origin) to both ends of the paired-end reads in the FASTQ file. After demultiplexing, the pipeline proceeds to use Trimmomatic v0.32, an open source tool to trim poor quality feeds from the reads. It uses a sliding window trimming, that cuts out sequences when the average quality within a window falls below a certain threshold. Finally, Bowtie 2.1.0 is used to align the reads to existing mitochondrial and phiX sequences and to remove them.
	
	% section introduction (end)
	\section{Data Management} % (fold)
	\label{sec:data_management}
	This 
	Talk about the db schema, rationale.
	The database is initialized with 4 tables:

	TODO - NEED TO CONSULT JC for clarification on db structure
	* primer - holds all the primers used in experiment
	* offset - randomized offsets to aid 16S sequencing
	* barcode - experiment identifier
	* log - holds logging data to aid accountability in experiments

	The log table supports the logging of metadata and a record of all operations done on the database.
	How to describe metadata logging? What sort of metadata are we planning to capture?

	General logging can be described in accountability terms - e.g. time of operation, tables operated on,
	number of rows operated, who (email? username?) operated. Purpose of this table
	is to automatically record operations so experiments can be reproduced with the
	appropriate settings/parameters easily.

	During the process of importing FASTQ data into the database, another table is produced:

	* reads1,reads2 - holds paired-end FASTQ data
	* reads - holds single-end FASTQ data
	
	\subsection{Compression} % (fold)
	\label{sub:compression}
	Short description of the compression scheme (combining sequence and quality lines)
	and file size savings

	Majority of space usage within sequence/quality lines:
	\begin{verbatim}
		@HWI-ST0747:277:D1M96ACXX:6:1101:1232:2090 1:N:0:
		GGATAGTACTAGGGTATCTAATCCTGTTTGCTCCCCACGCTTTCGCACCTCAGCGTCAGTATCGAGCCAGTGAGCCGCCTTCGCCACTGGTGTTCCTCCGAATATCTACGAATTTCACTGCTACACGCGGAATTCCATCCCCCTCTACCGT
		+
		ACCCFFDDFH#FHHIGIJJFJJJJJJJJJJJIIJJJGIJJGIJJJIGIJJJIIIIJJJJHHHHHFFFDCEACCDDCDDD@BDDDDBDDDDDDDDDDDDDBBB@BDEEACDDDDDDDEDDCCDDDADBBDDDDDDDDECCBDDDB@9@AA<<
		
		ATCGN #$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ
	\end{verbatim}




	* 8-bit char can hold 256 (28) values
	* 5 bases require 4 (22) to 8 (23) values
	* 42 qualities require 32 (25) to 64 (26) values

	A: 0-49
	T: 50-99
	C: 100-149
	G: 150-199
	N: 200-255

	* 50% compression
	* Fast compression/decompression
	* Lossless
	% subsection compression (end)
	% section data_management (end)
\section{Data Flow using UNIX Pipes} % (fold)
\label{sec:data_flow_using_unix_pipes}
A central part of managing the sequence data in the 16S pipeline is the manipulation
and transferring of sequences between the various tools. There is often a need
to modify the structure or format of the data to fit the varying input requirements
of those tools. Table~\ref{tab:expected_input} shows the various input formats that
tools in the pipeline expect.

\begin{table}
	\begin{tabular}{cc}
	\hline
	Tool & Expected input format\\
	\hline
	Trimmomatic & FASTQ/Gzipped FASTQ\\
	\hline
	Bowtie2 & FASTQ/FASTA\\
	\hline
	QIIME & 454-FASTA/454-Quality Scores\\
	\hline
	\end{tabular}
	\caption{Expected input formats for various tools}
	\label{tab:expected_input}
\end{table}

Traditionally, the way to manage the various formats was to transform and store
various copies of the data in multiple files. For example, running a sequence through
just the first three tools in the pipeline will produce the following files:



* Streaming Data with Named Pipes
	* Tool-specific adapters
* Demultiplexer, Trimmomatic, Bowtie
* No intermediate files Virtual files on filesystem

1. Request data
2. Transforms data into streams of FASTQ data
3. Feed data into tool using pipes
4. Receive feedback from tool
5. Transform feedback into deltas
6. Store data into SQLite

* No intermediate files
* No disk I/O

% section data_flow_using_unix_pipes (end)
\section{Benchmarks} % (fold)
\label{sec:benchmarks}
Show insert speed, normalization speed, streaming speed (to trimmomatic)
\begin{table}[h]
\centering
\begin{tabular}{n{4}{2}n{4}{2}}
	\toprule
 {Number of sequences (paired-end)} & {Time taken (seconds)} \\
 \midrule
 \np{500000} & 10.35 \\
 \np{1000000} & 19.57 \\
 \np{2000000} & 39.22 \\
 \np{4000000} & 78.67 \\
 \np{8000000} & 160.84 \\
 \np{16000000} & 319.38 \\
 \np{32000000} & 644.90 \\
 \np{64000000} & 1286.26 \\
 \bottomrule
\end{tabular}
\caption{Insertion speeds}
\label{tab:insertion_speeds}
\end{table}

\begin{figure}[h!]
	
	\centering
	\includegraphics[width=\textwidth]{insertion_speed_chart}
	\caption{Chart of insertion speeds}
\end{figure}


% section benchmarks (end)
\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)
\end{document}